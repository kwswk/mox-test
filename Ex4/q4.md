
Assume that 
- Airflow is deployed as primary workflow management solution
- Data are stored in AWS S3

1. Set up a Dag named "test" with schedule interval equal to 30 minutes
2. The whole dag should be compose of a few tasks:
   1. detect_file_arrival: With use of S3 Hook, list and filter keys which modify time is greater than the Airflow {{execution_time}}. Store the list of files in task instance using XCOM
   2. file_zip: Use Python operator to conduct below tasks:
      - Create create corresponding gzip file
      - write late arrival csv into existing gzip
      - upload updated gzip to target
3. The other jobs that depends on the output should be configurated like:
   - Use S3 sensor to detect existance of file
   - 
4. To handle new defined event in the future, we could make the dag "dynamic" by getting the folder s3 prefix as a event list and allow airflow to create tasks dynamically
